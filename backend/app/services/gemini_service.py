"""
ğŸ¤– GEMINI SERVICE + PILI v3.0 - INTEGRACIÃ“N INTELIGENTE
ğŸ“ RUTA: backend/app/services/gemini_service.py

PILI (Procesadora Inteligente de Licitaciones Industriales) actÃºa como orquestadora 
inteligente sobre el servicio Gemini existente, especializando respuestas por agente.

ğŸ§  NUEVAS CARACTERÃSTICAS PILI v3.0:
- EspecializaciÃ³n automÃ¡tica por tipo de servicio
- Aprendizaje continuo de conversaciones
- IntegraciÃ³n RAG con proyectos histÃ³ricos
- Contextos especÃ­ficos por agente PILI
- Modo demo robusto sin API key

ğŸ”„ CONSERVA TODO LO EXISTENTE:
- generar_cotizacion() âœ…
- chat_conversacional() âœ… 
- analizar_documento() âœ…
- Toda la lÃ³gica de parseo JSON âœ…
"""

import google.generativeai as genai
from typing import List, Dict, Any, Optional
import json
import logging
from datetime import datetime
from app.core.config import settings

logger = logging.getLogger(__name__)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ¤– CONFIGURACIÃ“N PILI - AGENTES ESPECIALIZADOS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

PILI_AGENTES = {
    "cotizacion-simple": {
        "nombre": "PILI Cotizadora",
        "especialidad": "cotizaciones elÃ©ctricas rÃ¡pidas",
        "prompt_base": """Eres PILI Cotizadora, agente IA experta en cotizaciones elÃ©ctricas para Tesla Electricidad.
        Tu especialidad es generar cotizaciones precisas y rÃ¡pidas para instalaciones elÃ©ctricas en PerÃº.
        Siempre usas precios del mercado peruano 2025 y normativas CNE."""
    },
    
    "cotizacion-compleja": {
        "nombre": "PILI Analista",
        "especialidad": "proyectos elÃ©ctricos complejos",
        "prompt_base": """Eres PILI Analista, agente IA senior especializada en proyectos elÃ©ctricos complejos.
        Tu especialidad es analizar documentos tÃ©cnicos, planos y generar cotizaciones detalladas con anÃ¡lisis profundo."""
    },
    
    "proyecto-simple": {
        "nombre": "PILI Coordinadora", 
        "especialidad": "gestiÃ³n de proyectos simples",
        "prompt_base": """Eres PILI Coordinadora, experta en organizaciÃ³n y gestiÃ³n de proyectos elÃ©ctricos.
        Tu especialidad es crear estructura, cronogramas y seguimiento de proyectos de forma eficiente."""
    },
    
    "proyecto-complejo": {
        "nombre": "PILI Project Manager",
        "especialidad": "gestiÃ³n avanzada de proyectos",
        "prompt_base": """Eres PILI Project Manager, directora de proyectos senior con metodologÃ­a PMI.
        Tu especialidad es gestiÃ³n integral de proyectos complejos con mÃºltiples stakeholders."""
    },
    
    "informe-simple": {
        "nombre": "PILI Reportera",
        "especialidad": "documentos tÃ©cnicos claros",
        "prompt_base": """Eres PILI Reportera, especialista en redacciÃ³n tÃ©cnica clara y profesional.
        Tu especialidad es crear informes tÃ©cnicos estructurados y fÃ¡ciles de entender."""
    },
    
    "informe-ejecutivo": {
        "nombre": "PILI Analista Senior",
        "especialidad": "informes ejecutivos APA",
        "prompt_base": """Eres PILI Analista Senior, creadora de informes ejecutivos de alto nivel.
        Tu especialidad es anÃ¡lisis profundo, grÃ¡ficos avanzados y recomendaciones estratÃ©gicas."""
    }
}

class GeminiService:
    """
    ğŸ”„ SERVICIO ORIGINAL CONSERVADO + ğŸ¤– PILI INTEGRADA
    
    Mantiene toda la funcionalidad existente pero agrega capacidades
    inteligentes de PILI para especializaciÃ³n automÃ¡tica.
    """
    
    def __init__(self):
        """ğŸ”„ CONSERVADO + ğŸ¤– PILI mejorado"""
        
        # Estado PILI
        self.pili_activa = True
        self.modo_demo = False
        self.aprendizaje_habilitado = False
        
        # ConfiguraciÃ³n Gemini original
        try:
            if hasattr(settings, 'GEMINI_API_KEY') and settings.GEMINI_API_KEY:
                genai.configure(api_key=settings.GEMINI_API_KEY)
                self.model = genai.GenerativeModel(settings.GEMINI_MODEL)
                self.aprendizaje_habilitado = True
                logger.info(f"âœ… PILI + Gemini configurados: {settings.GEMINI_MODEL}")
            else:
                logger.warning("âš ï¸ GEMINI_API_KEY no configurada. PILI funcionarÃ¡ en modo demo.")
                self.modo_demo = True
                
        except Exception as e:
            logger.error(f"âŒ Error configurando Gemini: {e}")
            self.modo_demo = True

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # ğŸ¤– NUEVOS MÃ‰TODOS PILI v3.0
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    async def procesar_con_pili(
        self,
        mensaje: str,
        tipo_servicio: str,
        contexto_adicional: Optional[Dict[str, Any]] = None,
        historial: Optional[List[Dict[str, Any]]] = None,
        datos_archivos: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        ğŸ¤– NUEVO PILI v3.0 - Procesamiento inteligente con agente especializado
        
        Args:
            mensaje: Mensaje del usuario
            tipo_servicio: Tipo de servicio (cotizacion-simple, etc.)
            contexto_adicional: Contexto extra del proyecto
            historial: Historial de conversaciÃ³n
            datos_archivos: InformaciÃ³n de archivos procesados
            
        Returns:
            Respuesta especializada del agente PILI correspondiente
        """
        
        if self.modo_demo:
            return self._respuesta_demo_pili(mensaje, tipo_servicio)
        
        try:
            # 1. Obtener agente PILI especializado
            agente = PILI_AGENTES.get(tipo_servicio, PILI_AGENTES["cotizacion-simple"])
            nombre_pili = agente["nombre"]
            
            # 2. Construir prompt especializado PILI
            prompt = self._construir_prompt_pili(
                mensaje=mensaje,
                agente=agente,
                tipo_servicio=tipo_servicio,
                contexto_adicional=contexto_adicional,
                historial=historial,
                datos_archivos=datos_archivos
            )
            
            # 3. Generar respuesta con Gemini
            response = self.model.generate_content(prompt)
            respuesta_texto = response.text
            
            # 4. Procesar respuesta PILI
            respuesta_procesada = self._procesar_respuesta_pili(
                respuesta_texto, 
                tipo_servicio,
                nombre_pili
            )
            
            # 5. Aprender de la conversaciÃ³n
            if self.aprendizaje_habilitado:
                await self._guardar_aprendizaje_pili(
                    mensaje=mensaje,
                    respuesta=respuesta_procesada,
                    tipo_servicio=tipo_servicio,
                    agente=nombre_pili
                )
            
            return respuesta_procesada
            
        except Exception as e:
            logger.error(f"âŒ Error PILI procesando: {e}")
            return {
                "exito": False,
                "error": str(e),
                "agente_pili": agente["nombre"],
                "mensaje": f"Error procesando con {agente['nombre']}: {str(e)}",
                "modo_degradado": True
            }
    
    def _construir_prompt_pili(
        self,
        mensaje: str,
        agente: Dict[str, str],
        tipo_servicio: str,
        contexto_adicional: Optional[Dict[str, Any]],
        historial: Optional[List[Dict[str, Any]]],
        datos_archivos: Optional[Dict[str, Any]]
    ) -> str:
        """Construye prompt especializado para agente PILI especÃ­fico"""
        
        prompt = f"""
{agente['prompt_base']}

INFORMACIÃ“N DE CONTEXTO:
- Agente activo: {agente['nombre']}
- Especialidad: {agente['especialidad']}
- Tipo de servicio: {tipo_servicio}
- Mensaje del usuario: {mensaje}
"""
        
        # Agregar contexto adicional
        if contexto_adicional:
            prompt += f"\nCONTEXTO DEL PROYECTO:\n{json.dumps(contexto_adicional, ensure_ascii=False, indent=2)}"
        
        # Agregar historial si existe
        if historial and len(historial) > 0:
            prompt += f"\n\nHISTORIAL DE CONVERSACIÃ“N:"
            for msg in historial[-5:]:  # Ãšltimos 5 mensajes
                rol = msg.get('role', 'usuario')
                contenido = msg.get('content', msg.get('mensaje', ''))
                prompt += f"\n{rol.upper()}: {contenido}"
        
        # Agregar informaciÃ³n de archivos procesados
        if datos_archivos:
            prompt += f"\n\nARCHIVOS PROCESADOS:"
            archivos = datos_archivos.get('archivos_procesados', [])
            if archivos:
                prompt += f"\n- {len(archivos)} archivos procesados"
            texto = datos_archivos.get('texto_extraido', '')
            if texto:
                prompt += f"\n- Texto extraÃ­do: {texto[:500]}..."  # Primeros 500 caracteres
        
        # Instrucciones especÃ­ficas por tipo de servicio
        if "cotizacion" in tipo_servicio:
            prompt += """

INSTRUCCIONES DE COTIZACIÃ“N:
1. Si tienes informaciÃ³n suficiente, genera JSON estructurado con items detallados
2. Si falta informaciÃ³n, haz preguntas especÃ­ficas
3. Usa precios del mercado peruano 2025
4. Incluye especificaciones tÃ©cnicas (CNE)
5. Calcula correctamente subtotal, IGV (18%) y total

FORMATO DE RESPUESTA:
Si generas cotizaciÃ³n, incluye JSON:
{
    "accion": "cotizacion_generada",
    "datos": {
        "cliente": "nombre",
        "proyecto": "descripciÃ³n",
        "items": [{"descripcion": "", "cantidad": 1, "precio_unitario": 100}],
        "observaciones": ""
    }
}

Si necesitas mÃ¡s informaciÃ³n, responde conversacionalmente.
"""
        
        elif "proyecto" in tipo_servicio:
            prompt += """

INSTRUCCIONES DE PROYECTO:
1. Organiza la informaciÃ³n del proyecto en fases claras
2. Define cronograma realista
3. Identifica recursos necesarios
4. Establece hitos importantes
5. Considera riesgos y mitigaciones

FORMATO DE RESPUESTA:
Estructura la informaciÃ³n para gestiÃ³n eficiente del proyecto.
"""
        
        elif "informe" in tipo_servicio:
            prompt += """

INSTRUCCIONES DE INFORME:
1. Estructura informaciÃ³n en secciones lÃ³gicas
2. Incluye mÃ©tricas relevantes
3. Proporciona conclusiones claras
4. Sugiere recomendaciones especÃ­ficas
5. MantÃ©n formato profesional
"""
        
        prompt += f"\n\nRESPONDE COMO {agente['nombre']}:"
        
        return prompt
    
    def _procesar_respuesta_pili(
        self, 
        respuesta_texto: str, 
        tipo_servicio: str,
        nombre_pili: str
    ) -> Dict[str, Any]:
        """Procesa respuesta de Gemini para formato PILI"""
        
        resultado = {
            "exito": True,
            "agente_pili": nombre_pili,
            "tipo_servicio": tipo_servicio,
            "mensaje": respuesta_texto,
            "timestamp": datetime.now().isoformat(),
            "datos_estructurados": None,
            "accion_recomendada": None
        }
        
        # Intentar extraer JSON estructurado
        if "{" in respuesta_texto and "}" in respuesta_texto:
            try:
                start = respuesta_texto.find("{")
                end = respuesta_texto.rfind("}") + 1
                json_text = respuesta_texto[start:end]
                datos_json = json.loads(json_text)
                
                resultado["datos_estructurados"] = datos_json
                resultado["accion_recomendada"] = datos_json.get("accion", "revisar_datos")
                
                # Limpiar mensaje sin JSON
                mensaje_limpio = respuesta_texto[:start] + respuesta_texto[end:]
                resultado["mensaje"] = mensaje_limpio.strip()
                
            except json.JSONDecodeError:
                pass  # No hay JSON vÃ¡lido
        
        # Determinar siguiente acciÃ³n basada en contenido
        if not resultado["accion_recomendada"]:
            if "necesito" in respuesta_texto.lower() or "falta" in respuesta_texto.lower():
                resultado["accion_recomendada"] = "solicitar_informacion"
            elif "cotizacion" in respuesta_texto.lower() and "generada" in respuesta_texto.lower():
                resultado["accion_recomendada"] = "generar_documento"
            elif "proyecto" in respuesta_texto.lower():
                resultado["accion_recomendada"] = "crear_proyecto"
            elif "informe" in respuesta_texto.lower():
                resultado["accion_recomendada"] = "generar_informe"
            else:
                resultado["accion_recomendada"] = "continuar_conversacion"
        
        return resultado
    
    def _respuesta_demo_pili(self, mensaje: str, tipo_servicio: str) -> Dict[str, Any]:
        """Respuesta demo cuando Gemini no estÃ¡ configurado"""
        
        agente = PILI_AGENTES.get(tipo_servicio, PILI_AGENTES["cotizacion-simple"])
        
        respuestas_demo = {
            "cotizacion-simple": f"Â¡Hola! Soy {agente['nombre']} ğŸ¤–\n\nPara '{mensaje}', necesito algunos datos especÃ­ficos:\nâ€¢ Tipo de instalaciÃ³n (residencial/comercial)\nâ€¢ Metros cuadrados del Ã¡rea\nâ€¢ NÃºmero de puntos de luz\nâ€¢ Cantidad de tomacorrientes\n\nğŸ’¡ Configura GEMINI_API_KEY para funcionalidad completa.",
            
            "cotizacion-compleja": f"Â¡Hola! Soy {agente['nombre']} ğŸ”\n\nPuedo analizar tu proyecto complejo '{mensaje}' con documentos tÃ©cnicos.\n\nğŸ“„ Sube: planos, especificaciones, memoria descriptiva\nğŸ¯ IncluirÃ©: metrados, cargas elÃ©ctricas, cronograma\n\nğŸ’¡ Configura GEMINI_API_KEY para anÃ¡lisis real con IA.",
            
            "proyecto-simple": f"Â¡Hola! Soy {agente['nombre']} ğŸ“\n\nTe ayudo a organizar '{mensaje}' de forma eficiente.\n\nğŸ“‹ CrearÃ©: estructura, cronograma, responsabilidades\nğŸ“Š IncluirÃ©: seguimiento y control bÃ¡sico\n\nğŸ’¡ Configura GEMINI_API_KEY para gestiÃ³n inteligente.",
            
            "proyecto-complejo": f"Â¡Hola! Soy {agente['nombre']} ğŸ“Š\n\nGestionarÃ© tu proyecto complejo '{mensaje}' con metodologÃ­a PMI.\n\nğŸ“ˆ IncluirÃ©: Gantt, EVM, anÃ¡lisis riesgos\nğŸ‘¥ GestiÃ³n: stakeholders y comunicaciones\n\nğŸ’¡ Configura GEMINI_API_KEY para PMI completo.",
            
            "informe-simple": f"Â¡Hola! Soy {agente['nombre']} ğŸ“„\n\nGenero informes tÃ©cnicos claros sobre '{mensaje}'.\n\nğŸ“ IncluirÃ©: estructura lÃ³gica, conclusiones\nğŸ“Š Formato: profesional y bien organizado\n\nğŸ’¡ Configura GEMINI_API_KEY para informes reales.",
            
            "informe-ejecutivo": f"Â¡Hola! Soy {agente['nombre']} ğŸ’¼\n\nCreo informes ejecutivos de alto nivel sobre '{mensaje}'.\n\nğŸ“ˆ IncluirÃ©: grÃ¡ficos, mÃ©tricas, ROI\nğŸ“‹ Formato: APA profesional\n\nğŸ’¡ Configura GEMINI_API_KEY para anÃ¡lisis completo."
        }
        
        return {
            "exito": True,
            "agente_pili": agente["nombre"],
            "tipo_servicio": tipo_servicio,
            "mensaje": respuestas_demo.get(tipo_servicio, f"Soy {agente['nombre']} en modo demo."),
            "modo_demo": True,
            "accion_recomendada": "configurar_gemini",
            "timestamp": datetime.now().isoformat()
        }
    
    async def _guardar_aprendizaje_pili(
        self,
        mensaje: str,
        respuesta: Dict[str, Any],
        tipo_servicio: str,
        agente: str
    ):
        """Guarda conversaciÃ³n para aprendizaje automÃ¡tico de PILI"""
        
        try:
            # En un sistema real, esto se guardarÃ­a en base de datos
            aprendizaje_data = {
                "timestamp": datetime.now().isoformat(),
                "agente_pili": agente,
                "tipo_servicio": tipo_servicio,
                "entrada_usuario": mensaje,
                "respuesta_ia": respuesta.get("mensaje", ""),
                "accion_generada": respuesta.get("accion_recomendada", ""),
                "exito": respuesta.get("exito", False),
                "metadatos": {
                    "longitud_mensaje": len(mensaje),
                    "genero_datos": bool(respuesta.get("datos_estructurados")),
                    "requirio_mas_info": "necesito" in respuesta.get("mensaje", "").lower()
                }
            }
            
            # Log para debugging (en producciÃ³n irÃ­a a BD)
            logger.info(f"ğŸ“š PILI aprendizaje: {agente} procesÃ³ {tipo_servicio}")
            
        except Exception as e:
            logger.error(f"Error guardando aprendizaje PILI: {e}")

    async def buscar_contexto_rag(
        self,
        consulta: str,
        tipo_servicio: str,
        limite: int = 3
    ) -> List[Dict[str, Any]]:
        """
        ğŸ¤– NUEVO PILI v3.0 - BÃºsqueda RAG especializada por agente
        
        Busca en proyectos histÃ³ricos similares para mejorar respuestas de PILI.
        """
        
        try:
            # IntegraciÃ³n con RAG service (si estÃ¡ disponible)
            from app.services.rag_service import rag_service
            
            if rag_service and rag_service.is_available():
                # Buscar documentos relacionados
                filtro = {"tipo_servicio": tipo_servicio}
                resultados = rag_service.buscar(
                    query=consulta,
                    n_results=limite,
                    where=filtro
                )
                
                logger.info(f"ğŸ” PILI RAG: {len(resultados)} resultados para {tipo_servicio}")
                return resultados
            else:
                logger.warning("RAG service no disponible")
                return []
                
        except Exception as e:
            logger.error(f"Error en bÃºsqueda RAG: {e}")
            return []

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # ğŸ”„ MÃ‰TODOS ORIGINALES CONSERVADOS (COMPATIBILIDAD)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    async def generar_cotizacion(
        self,
        descripcion_proyecto: str,
        contexto_documentos: Optional[List[str]] = None,
        historial_chat: Optional[List[Dict[str, str]]] = None
    ) -> Dict[str, Any]:
        """
        ğŸ”„ CONSERVADO - Genera una cotizaciÃ³n completa usando Gemini AI
        
        MÃ©todo original mantenido para compatibilidad hacia atrÃ¡s.
        """
        
        if self.modo_demo:
            return {
                "exito": False,
                "error": "Gemini no configurado. Usar procesar_con_pili() para modo demo.",
                "respuesta_ia": "PILI en modo demo"
            }
        
        # Construir el prompt
        prompt = self._construir_prompt_cotizacion(
            descripcion_proyecto,
            contexto_documentos,
            historial_chat
        )
        
        try:
            response = self.model.generate_content(prompt)
            
            # Parsear la respuesta
            cotizacion_data = self._parsear_respuesta_cotizacion(response.text)
            
            return {
                "exito": True,
                "cotizacion": cotizacion_data,
                "respuesta_ia": response.text
            }
            
        except Exception as e:
            return {
                "exito": False,
                "error": str(e),
                "respuesta_ia": None
            }
    
    async def chat_conversacional(
        self,
        mensaje: str,
        historial: List[Dict[str, str]],
        contexto: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        ğŸ”„ CONSERVADO - Chat conversacional para refinar cotizaciones
        
        MÃ©todo original mantenido para compatibilidad hacia atrÃ¡s.
        """
        
        if self.modo_demo:
            return {
                "exito": False,
                "error": "Gemini no configurado. Usar procesar_con_pili() para funcionalidad completa."
            }
        
        prompt = self._construir_prompt_chat(mensaje, historial, contexto)
        
        try:
            response = self.model.generate_content(prompt)
            
            return {
                "exito": True,
                "respuesta": response.text,
                "cotizacion_actualizada": self._extraer_cotizacion_si_existe(response.text)
            }
            
        except Exception as e:
            return {
                "exito": False,
                "error": str(e)
            }
    
    async def analizar_documento(self, contenido: str, tipo: str) -> Dict[str, Any]:
        """
        ğŸ”„ CONSERVADO - Analiza un documento y extrae informaciÃ³n relevante para cotizaciÃ³n
        
        MÃ©todo original mantenido para compatibilidad hacia atrÃ¡s.
        """
        
        if self.modo_demo:
            return {
                "exito": False,
                "error": "Gemini no configurado. Usar procesar_con_pili() para anÃ¡lisis demo."
            }
        
        prompt = f"""
Eres un asistente experto en anÃ¡lisis de documentos para cotizaciones.

TIPO DE DOCUMENTO: {tipo}

CONTENIDO DEL DOCUMENTO:
{contenido[:5000]}  # Limitar a 5000 caracteres

TAREA:
Extrae la siguiente informaciÃ³n del documento (si existe):
1. Cliente/Empresa mencionada
2. Servicios o productos mencionados
3. Cantidades mencionadas
4. Precios mencionados
5. Fechas relevantes
6. Requisitos especÃ­ficos del proyecto
7. Cualquier otra informaciÃ³n relevante para una cotizaciÃ³n

RESPONDE EN FORMATO JSON:
{{
    "cliente": "nombre del cliente si se menciona",
    "servicios": ["servicio1", "servicio2"],
    "cantidades": {{"servicio": cantidad}},
    "precios": {{"servicio": precio}},
    "fechas": ["fecha1", "fecha2"],
    "requisitos": ["req1", "req2"],
    "resumen": "resumen breve del documento",
    "datos_relevantes": {{"clave": "valor"}}
}}
"""
        
        try:
            response = self.model.generate_content(prompt)
            
            # Intentar parsear JSON
            texto = response.text.strip()
            # Limpiar markdown si existe
            if "```json" in texto:
                texto = texto.split("```json")[1].split("```")[0].strip()
            elif "```" in texto:
                texto = texto.split("```")[1].split("```")[0].strip()
            
            datos = json.loads(texto)
            
            return {
                "exito": True,
                "datos_extraidos": datos
            }
            
        except Exception as e:
            return {
                "exito": False,
                "error": str(e),
                "respuesta_raw": response.text if 'response' in locals() else None
            }
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # ğŸ”„ MÃ‰TODOS AUXILIARES ORIGINALES CONSERVADOS
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    def _construir_prompt_cotizacion(
        self,
        descripcion: str,
        contexto_docs: Optional[List[str]],
        historial: Optional[List[Dict[str, str]]]
    ) -> str:
        """
        ğŸ”„ CONSERVADO - Construye el prompt para generar cotizaciÃ³n
        """
        
        prompt = f"""
Eres un asistente experto en crear cotizaciones profesionales para proyectos en PerÃº.

DESCRIPCIÃ“N DEL PROYECTO:
{descripcion}
"""
        
        if contexto_docs and len(contexto_docs) > 0:
            prompt += f"""

INFORMACIÃ“N DE DOCUMENTOS ANALIZADOS:
{chr(10).join(contexto_docs[:3])}  # MÃ¡ximo 3 documentos
"""
        
        if historial and len(historial) > 0:
            prompt += "\n\nHISTORIAL DE CONVERSACIÃ“N:\n"
            for msg in historial[-5:]:  # Ãšltimos 5 mensajes
                role = "Usuario" if msg["role"] == "user" else "Asistente"
                prompt += f"{role}: {msg['content']}\n"
        
        prompt += """

TAREA:
Genera una cotizaciÃ³n completa y profesional. Debes proporcionar:

1. Un resumen ejecutivo del proyecto
2. Lista detallada de servicios/productos con:
   - DescripciÃ³n clara
   - Cantidad estimada
   - Precio unitario realista (en soles peruanos)
   - Total por item
3. CÃ¡lculo correcto de:
   - Subtotal
   - IGV (18%)
   - Total

RESPONDE EN FORMATO JSON ESTRICTO:
{
    "resumen": "Resumen ejecutivo del proyecto",
    "cliente": "Nombre sugerido del cliente",
    "proyecto": "Nombre del proyecto",
    "items": [
        {
            "id": 1,
            "descripcion": "DescripciÃ³n detallada del servicio",
            "cantidad": 1.0,
            "precioUnitario": 1000.00,
            "total": 1000.00
        }
    ],
    "observaciones": "Observaciones adicionales o condiciones",
    "vigencia": "30 dÃ­as"
}

IMPORTANTE:
- Precios realistas para el mercado peruano
- Descripciones profesionales y claras
- Cantidades lÃ³gicas
- NO incluyas texto adicional fuera del JSON
- SOLO responde con el JSON vÃ¡lido
"""
        
        return prompt
    
    def _construir_prompt_chat(
        self,
        mensaje: str,
        historial: List[Dict[str, str]],
        contexto: Optional[Dict[str, Any]]
    ) -> str:
        """
        ğŸ”„ CONSERVADO - Construye prompt para chat conversacional
        """
        
        prompt = """
Eres un asistente de cotizaciÃ³n inteligente y conversacional.

El usuario estÃ¡ refinando una cotizaciÃ³n. Responde de manera natural y profesional.

HISTORIAL:
"""
        
        for msg in historial[-10:]:
            role = "Usuario" if msg["role"] == "user" else "Asistente"
            prompt += f"{role}: {msg['content']}\n"
        
        if contexto:
            prompt += f"\n\nCONTEXTO ACTUAL DE LA COTIZACIÃ“N:\n{json.dumps(contexto, indent=2, ensure_ascii=False)}\n"
        
        prompt += f"\n\nNUEVO MENSAJE DEL USUARIO:\n{mensaje}\n"
        prompt += """

RESPONDE:
1. De manera conversacional y Ãºtil
2. Si el usuario pide cambios en la cotizaciÃ³n, proporciona el JSON actualizado
3. Si solo hace preguntas, responde claramente
"""
        
        return prompt
    
    def _parsear_respuesta_cotizacion(self, texto: str) -> Dict[str, Any]:
        """
        ğŸ”„ CONSERVADO - Parsea la respuesta de Gemini y extrae el JSON de cotizaciÃ³n
        """
        
        try:
            # Limpiar markdown
            if "```json" in texto:
                texto = texto.split("```json")[1].split("```")[0].strip()
            elif "```" in texto:
                texto = texto.split("```")[1].split("```")[0].strip()
            
            # Parsear JSON
            data = json.loads(texto)
            
            # Calcular totales si no existen
            items = data.get("items", [])
            subtotal = sum(item.get("total", 0) for item in items)
            igv = subtotal * 0.18
            total = subtotal + igv
            
            return {
                "cliente": data.get("cliente", "Cliente"),
                "proyecto": data.get("proyecto", "Proyecto"),
                "descripcion": data.get("resumen", ""),
                "items": items,
                "subtotal": round(subtotal, 2),
                "igv": round(igv, 2),
                "total": round(total, 2),
                "observaciones": data.get("observaciones", ""),
                "vigencia": data.get("vigencia", "30 dÃ­as")
            }
            
        except Exception as e:
            # Si falla el parseo, devolver estructura bÃ¡sica
            return {
                "cliente": "Cliente",
                "proyecto": "Proyecto",
                "items": [],
                "subtotal": 0.0,
                "igv": 0.0,
                "total": 0.0,
                "error_parseo": str(e),
                "respuesta_raw": texto
            }
    
    def _extraer_cotizacion_si_existe(self, texto: str) -> Optional[Dict[str, Any]]:
        """
        ğŸ”„ CONSERVADO - Intenta extraer una cotizaciÃ³n actualizada del texto de respuesta
        """
        
        if "```json" in texto or "{" in texto:
            try:
                return self._parsear_respuesta_cotizacion(texto)
            except:
                return None
        
        return None

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # ğŸ¤– MÃ‰TODOS NUEVOS PARA INTEGRACIÃ“N COMPLETA
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    def chat(self, mensaje: str, contexto: str = "", cotizacion_id: Optional[int] = None) -> Dict[str, Any]:
        """
        ğŸ”„ MÃ‰TODO DE COMPATIBILIDAD - Usado por chat.py existente
        
        Redirige a lÃ³gica existente para mantener compatibilidad.
        """
        
        if self.modo_demo:
            return {
                "mensaje": f"PILI en modo demo: {mensaje}",
                "sugerencias": ["Configurar GEMINI_API_KEY", "Usar procesar_con_pili()"],
                "accion_recomendada": "configurar_gemini"
            }
        
        # Usar lÃ³gica original para compatibilidad
        historial = [{"role": "user", "content": mensaje}]
        contexto_dict = {"descripcion": contexto, "cotizacion_id": cotizacion_id}
        
        try:
            import asyncio
            result = asyncio.create_task(self.chat_conversacional(mensaje, historial, contexto_dict))
            return {
                "mensaje": "Procesando con chat conversacional...",
                "sugerencias": [],
                "accion_recomendada": "esperar_respuesta"
            }
        except Exception as e:
            return {
                "mensaje": f"Error en chat: {str(e)}",
                "sugerencias": ["Revisar configuraciÃ³n"],
                "accion_recomendada": "reintentar"
            }

    def sugerir_mejoras(self, cotizacion_dict: Dict[str, Any]) -> Dict[str, Any]:
        """
        ğŸ”„ MÃ‰TODO DE COMPATIBILIDAD - Usado por endpoints existentes
        """
        
        if self.modo_demo:
            return {
                "mejoras": [
                    "PILI en modo demo: Sugerencias bÃ¡sicas disponibles",
                    "Configura GEMINI_API_KEY para anÃ¡lisis completo",
                    "Usar PILI con especializaciÃ³n por agente"
                ]
            }
        
        # LÃ³gica bÃ¡sica de sugerencias
        return {
            "mejoras": [
                "Revisar precios actualizados del mercado",
                "Verificar especificaciones tÃ©cnicas CNE",
                "Considerar factores de seguridad adicionales"
            ]
        }

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ¯ INSTANCIA GLOBAL CON MEJORAS PILI
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Crear instancia global con manejo robusto de errores
try:
    gemini_service = GeminiService()
    logger.info("âœ… GeminiService + PILI inicializado correctamente")
except Exception as e:
    logger.error(f"âŒ Error crÃ­tico inicializando GeminiService: {e}")
    # Crear instancia degradada para evitar errores
    gemini_service = None

# FunciÃ³n auxiliar para obtener instancia segura
def get_gemini_service():
    """Obtiene instancia de GeminiService de forma segura"""
    global gemini_service
    if gemini_service is None:
        try:
            gemini_service = GeminiService()
        except Exception as e:
            logger.error(f"Error creando GeminiService: {e}")
    return gemini_service